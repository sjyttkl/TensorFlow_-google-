一   , 零基础用TensorFlow玩转Kaggle的“手写识别”
Digit Recognizer.py:

  这个模型，组成是这样的：

1）使用一个最简单的单层的神经网络进行学习

2）用SoftMax来做为激活函数

3）用交叉熵来做损失函数

4）用梯度下降来做优化方式

这里有几个新的名词，神经网络、激活函数、SoftMax、损失函数、交叉熵、梯度下降，我们挨个解释一下。
神经网络：由很多个神经元组成，每个神经元接收很多个输入：[X1,X2....Xn]，
          加权相加然后加上偏移量后，看是不是超过了某个阀值，超过了发出1，没超过发出0。
激活函数：每个神经元，在通过一系列计算后，得到了一个数值，怎么来判断应该输出什么呢？激活函数就是解决这个问题，你把值给我，我来判断怎么输出。所以一个神经网络，激活函数是非常重要的。
          想要成为激活函数，你得有两把刷子啊。这两把刷子是：一是你得处处可微，可微分才能求导，求极值.
           二是要非线性的，因为线性模型的表达能力不够。

目前主流的几个激活函数是：sigmoid,tanh,ReLU。

sigmoid：采用S形函数，取值范围[0,1]

tanh：双切正切函数，取值范围[-1,1]

ReLU：简单而粗暴，大于0的留下，否则一律为0。

SoftMax：我们知道max(A,B)，�是指A和B里哪个大就取哪个值，但我们有时候希望比较小的那个也有一定概率取到，怎么办呢？
       我们就按照两个值的大小，计算出概率，按照这个概率来取A或者B。比如A=9，B=1,那取A的概率是90%，取B的概率是10%。
      这个看起来比max(A,B)这样粗暴的方式柔和一些，所以叫SoftMax（名字解释纯属个人瞎掰😑大家能理解概念就好）

损失函数：损失函数是模型对数据拟合程度的反映，拟合得越好损失应该越小，拟合越差损失应该越大，然后我们根据损失函数的结果
          对模型进行调整。
交叉熵：这个概念要解释的简单，那就不准确，如果要准确，那可能一千字都打不住。这里说一个简单但不一定准确的解释吧。
        比如，你想把乾坤大挪移练到第七层大圆满，你现在是第五层，那你还差两层，这个两层就是你和大圆满之间的距离。
        交叉熵通俗的讲就是现在的训练程度和圆满之间的距离，我们希望距离越小越好，所以交叉熵可以作为一个损失函数，来衡量和目标之间的距离。

梯度下降：这个概念可以这样理解，我们要解决的问题是一座山，答案在山底，我们从山顶到山底的过程就是解决问题的过程。
         在山顶，想找到最快的下山的路。这个时候，我们的做法是什么呢？在每次选择道路的时候，选最陡的那条路。
         梯度是改变率或者斜度的另一个称呼，用数学的语言解释是导数。对于求损失函数最小值这样的问题，朝着梯度下降的方向走，就能找到最优值了。

三、代码实现

1 载入数据，并对数据进行处理

在写代码的过程中，数据的预处理是最大的一块工作，做一个项目，60%以上的代码在做数据预处理。

这个项目的预处理，分为5步：

1）把输入和结果分开

2）对输入进行处理：把一维的输入变成28*28的矩阵

3）对结果进行处理：把结果进行One-Hot编码

4）把训练数据划分训练集和验证集

5）对训练集进行分批

作者：量化大司马
链接：https://www.jianshu.com/p/696bde1641d8
來源：简书
简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。



